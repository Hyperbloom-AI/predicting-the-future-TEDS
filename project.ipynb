{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substance Abuse Project\n",
    "\n",
    "To be able to predict the outcome of substance abuse treatment is a high value solution. A model being able to make the distinction between a passing and failing case is difficult and requires alot of information. For this iteration, we utlized the TEDs 2018 dataset provided by the **_Center for Behavioral Health Statistics and Quality_** and the **_Substance Abuse and Mental Health Services Administration_**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "If necessary please uncomment the following lines and install the dependencies with pip. Our notebook uses each one of these and it's important that all can be imported to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sagemaker\n",
    "# !pip install matplotlib\n",
    "# !pip install boto3\n",
    "# !pip install pandas\n",
    "# !pip install fsspec\n",
    "# !pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "For this section we work on removing uninmportant data, unlabelled rows, and modifying our dataset to be used with the XGBoost training algorithm.\n",
    "### Importing Dependencies and Setting Environment Variables\n",
    "In this next code block we import boto3 and pandas. Pandas will be important for our first step, importing the CSV file as a **dataframe** a sort of programatically accessible table that we can use in our python code. boto3 will become important in a later step when we need to export our split data back to s3. Though there are other popular dataframe modules such as dask, pandas remains the most open source with the best source documentation and clear instruction. It also features the fastest operation time. We also import math, json, numpy and matplotlib.pyplot for later use. Below that we set our environment variables which we use throughout the notebook.\n",
    "\n",
    "We also save our important filenames and the name of our bucket to the notebook for use later. Putting all this data together in one step means avoiding running extraneous code when rerunning code in later sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, pandas as pd, math, json, numpy as np, matplotlib.pyplot as plt, os\n",
    "\n",
    "bucket = 'bucket-sagemaker-substance-abuse'\n",
    "raw_file_2017 = 'data/raw/tedsd_puf_2017.csv'\n",
    "raw_file_2018 = 'data/raw/tedsd_puf_2018.csv'\n",
    "train_subfolder = 'data/train/'\n",
    "eval_subfolder = 'data/evaluation/'\n",
    "label = \"REASON\" # Desired Label\n",
    "objective = \"binary:logistic\" # binary:logistic or multi:softmax\n",
    "training_rounds = 10 # 1000-1500 for production, 10-100 for testing and development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our CSV as a pandas Dataframe Object\n",
    "Most of the lines in this block format a string for use with pandas, telling it where our file is located. I've opted for *join concatenation with a slash* instead of operator concatenation which uses more memory and is less efficient. The latter half of this block of code reads out the data from our source file to the pandas dataframe which we can mutate during our transformations. Any dataframes method with a possible `inplace` attribute should be set to true so that the dataframe is mutated and not returned except for in the case that you're saving it to a variable. \n",
    "\n",
    "This step may take a few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 Dataframe Loaded: 1666366 rows x 76 columns\n",
      "2017 Dataframe Loaded: 1661207 rows x 76 columns\n",
      "Dataframe Loaded: 3327573 rows x 76 columns\n"
     ]
    }
   ],
   "source": [
    "raw_location_2018 = 's3://{}/{}'.format(bucket, raw_file_2018)\n",
    "raw_location_2017 = 's3://{}/{}'.format(bucket, raw_file_2017)\n",
    "df_2018 = pd.read_csv(raw_location_2018)\n",
    "print(f\"2018 Dataframe Loaded: { len(df_2018) } rows x {len(df_2018.columns)} columns\")\n",
    "df_2017 = pd.read_csv(raw_location_2017)\n",
    "print(f\"2017 Dataframe Loaded: { len(df_2017) } rows x {len(df_2017.columns)} columns\")\n",
    "df = pd.concat([df_2018, df_2017])\n",
    "print(f\"Dataframe Loaded: { len(df) } rows x {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extraneous Columns\n",
    "In the next step we take the names of all the columns we want to remove and set them in an array. Originally we would use an array to loop through and drop them one by one however pandas `drop` function can take *list-like* parameters and so we can actually save both time and resources by doing it all at once. This will significantly increase development speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Transformed: 2571774 rows x 59 columns\n"
     ]
    }
   ],
   "source": [
    "remove = [\"CASEID\", \"CBSA2010\", \"EMPLOY_D\", \"DETNLF_D\", \"LIVARAG_D\", \"ARRESTS_D\", \"SERVICES_D\", \"SUB1_D\", \"FREQ1_D\", \"SUB2_D\", \"FREQ2_D\", \"SUB3_D\", \"FREQ3_D\", \"FREQ_ATND_SELF_HELP_D\", \"DIVISION\", \"LOS\", \"DISYR\"]\n",
    "index_names = df[ (df['REASON'] == 3) | (df['REASON'] == 5) | (df['REASON'] == 6) | (df['REASON'] == 7)].index # Collects Death, Incarceration, Termination, and Other labeled rows for removal\n",
    "df.drop(index_names, inplace = True) # Drops rows grabbed above\n",
    "df.drop(remove, inplace=True, axis=1) # Drops extraneous columns in 'remove' array above\n",
    "df.dropna(axis=0, subset=[label], inplace=True) # Drops rows missing desired label\n",
    "print(f\"Dataframe Transformed: { len(df) } rows x {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Categorical Data to Binary Columns\n",
    "<span style=\"color:red\">**BUG**</span> For this step we transform our categorical data into binary columns. To do this we call `get_dummies` on each column listed in the transform array as we loop through it. We then take that *dummy* dataframe and we concatenate it with the current dataframe along the vertical axis. After we loop through the entire array, we drop all the coulmns in the transform array as they're now redundant. Because there is a bug in this step, probably caused by a memory shortage or a CPU resource shortage, we should think about creating an empty dataframe in which we concatenate all dummies together and then concatenate that at the end instead of many times during the loop. In the last update I removed \"Reason\" from the transform list as we'll need to know what that becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Transformed: 2571774 rows x 306 columns\n"
     ]
    }
   ],
   "source": [
    "transform = [\"AGE\", \"GENDER\", \"RACE\", \"ETHNIC\", \"MARSTAT\", \"EDUC\", \"EMPLOY\", \"DETNLF\", \"PREG\", \"VET\", \"LIVARAG\", \"PRIMINC\", \"ARRESTS\", \"SERVICES\", \"METHUSE\", \"DAYWAIT\", \"PSOURCE\", \"DETCRIM\", \"NOPRIOR\", \"SUB1\", \"ROUTE1\", \"FREQ1\", \"FRSTUSE1\", \"SUB2\", \"ROUTE2\", \"FREQ2\", \"FRSTUSE2\", \"SUB3\", \"ROUTE3\", \"FREQ3\", \"FRSTUSE3\", \"IDU\", \"REGION\", \"STFIPS\"];\n",
    "df = pd.get_dummies(df, prefix=transform, columns=transform)\n",
    "print(f\"Dataframe Transformed: { len(df) } rows x {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Binary Classification and Modify Data for Algorithm\n",
    "This code moves our label to the front of the dataframe. It also decrements 1 from the Reason category so the the XDGBoost Algorithm can understand the classes. Our current categories are:\n",
    "- **0**: Positive Result\n",
    "- **1**: Non-positive Result\n",
    "\n",
    "For reasons having to do with confidence, we've phased out multi classification, though it can be reenabled by modifying the hyperparameters for the algorithm and modifying replace statements below. THe replace methods called may seem redundant but they are created in a way so that commenting out certain statements allows us to roll back to multi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels altered, class pool reduced to 2\n"
     ]
    }
   ],
   "source": [
    "df[label] = df[label].replace([3, 4], [2, 2]) # Converts to Binary\n",
    "column = df.pop(label)\n",
    "df.insert(0, label, column)\n",
    "df[label]-=1\n",
    "class_pool = pd.get_dummies(df[label]) # For Debugging\n",
    "print(f\"Labels altered, class pool reduced to {len(class_pool.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "To do proper model training we'll use a holdout dataset for evaluation and so we'll need to split the set into 80% and 20%\n",
    "### Separating Training and Evaluation Sets and Uploading them to S3\n",
    "To complete our tranformation section, we grab a random sample of our data that makes up half of our data and we transport it to S3 utilizing a boto3 s3 client after converting the new dataframe to a CSV. If the command is run twice it will overwrite previous data. This section also creates an Evaluation Dataframe and ships it in a CSV to a seperate folder from the training data.\n",
    "\n",
    "This process will take up to 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Dataframe Created: 2571774 rows x 306 columns\n",
      "Training Dataframe Created: 2057419 rows x 306 columns\n",
      "Evaluation Dataframe Created: 514354 rows x 306 columns\n",
      "Operation Complete, Files Uploaded\n"
     ]
    }
   ],
   "source": [
    "sdf = df.sample(frac=1)\n",
    "print(f\"Random Dataframe Created: { len(sdf) } rows x {len(sdf.columns)} columns\")\n",
    "training_end_index = math.floor(len(sdf)*0.8)\n",
    "eval_start_index = training_end_index + 1\n",
    "end = len(sdf) - 1\n",
    "tdf = sdf.iloc[0:training_end_index] # first five rows of dataframe\n",
    "print(f\"Training Dataframe Created: { len(tdf) } rows x {len(tdf.columns)} columns\")\n",
    "edf = sdf.iloc[training_end_index:end]\n",
    "print(f\"Evaluation Dataframe Created: { len(edf) } rows x {len(edf.columns)} columns\")\n",
    "\n",
    "t_filename = 'split-data/training_data.csv'\n",
    "e_filename = 'split-data/evaluation_data.csv'\n",
    "tdf.to_csv(t_filename, header=False, index=False)\n",
    "edf.to_csv(e_filename, header=False, index=False)\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file(t_filename, bucket, train_subfolder + t_filename)\n",
    "s3.meta.client.upload_file(e_filename, bucket, eval_subfolder + e_filename)\n",
    "print(f\"Operation Complete, Files Uploaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting local files\n",
      "local files deleted\n"
     ]
    }
   ],
   "source": [
    "print(f\"deleting local files\")\n",
    "os.remove(t_filename)\n",
    "os.remove(e_filename)\n",
    "print(f\"local files deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "### Setting Sagemaker Environment\n",
    "Now we get to important stuff. Here we leave the transformation phase and move into the training phase. This block saves our region into the notebook making it usable in our model. The steps below do not need to be run in conjunction with those above, provided you've already completed the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Model Parameters and HyperParameters\n",
    "In this section we set up our XDGBoost training and pass it our basic values and hyperparameters. `num_round` determines how many rounds of training the model goes through. `num_class` alters how many classes it accepts. If the objective changes to binary this can be removed. Using A/B Testing I was able to discover there's not really a point to doing more than *1000* rounds of training. The training m-error may keep dropping but the evaluation m-error stays roughly stagnant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:1.2-1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "model_bucket_prefix = 'output/models'\n",
    "\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, model_bucket_prefix, 'xgboost_model')\n",
    "\n",
    "container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "print(container)\n",
    "\n",
    "xgb_model=sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role = 'arn:aws:iam::620700586481:role/sagemaker-substance-abuse-project-role',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    volume_size=5,\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n",
    ")\n",
    "\n",
    "xgb_model.set_hyperparameters(\n",
    "    max_depth = 6,\n",
    "    eta = 0.3,\n",
    "    min_child_weight = 1,\n",
    "    subsample = 0.9,\n",
    "    objective = objective,\n",
    "    # num_class = 3,\n",
    "    num_round = training_rounds \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from S3 as Training Inputs\n",
    "Here we let SageMaker know what we intend to use for training and evaluationcand where it can find our CSV files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "prefix = \"data\"\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train/training_data.csv\"), content_type=\"csv\"\n",
    ")\n",
    "validation_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"evaluation/evaluation_data.csv\"), content_type=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "This part begins the training and the evaluation in one step. We pass it both our training input and our validation input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-12 18:59:42 Starting - Starting the training job...\n",
      "2021-08-12 19:00:06 Starting - Launching requested ML instancesCreateXgboostReport: InProgress\n",
      "ProfilerReport-1628794782: InProgress\n",
      "...\n",
      "2021-08-12 19:00:37 Starting - Preparing the instances for training.........\n",
      "2021-08-12 19:02:12 Downloading - Downloading input data......\n",
      "2021-08-12 19:03:11 Training - Downloading the training image...\n",
      "2021-08-12 19:03:38 Training - Training image download completed. Training in progress.\u001b[34m[2021-08-12 19:03:33.756 ip-10-0-193-51.us-east-2.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[2021-08-12 19:04:08.281 ip-10-0-193-51.us-east-2.compute.internal:1 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-08-12 19:04:08.282 ip-10-0-193-51.us-east-2.compute.internal:1 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-08-12 19:04:08.282 ip-10-0-193-51.us-east-2.compute.internal:1 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-08-12 19:04:08.283 ip-10-0-193-51.us-east-2.compute.internal:1 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-08-12 19:04:08.283 ip-10-0-193-51.us-east-2.compute.internal:1 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 2057419 rows and 305 columns\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 514354 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.28284#011validation-error:0.28267\u001b[0m\n",
      "\u001b[34m[2021-08-12 19:04:33.867 ip-10-0-193-51.us-east-2.compute.internal:1 INFO hook.py:413] Monitoring the collections: metrics, predictions, feature_importance, labels, hyperparameters\u001b[0m\n",
      "\u001b[34m[2021-08-12 19:04:33.869 ip-10-0-193-51.us-east-2.compute.internal:1 INFO hook.py:476] Hook is writing from the hook with pid: 1\n",
      "\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.27923#011validation-error:0.27952\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.27720#011validation-error:0.27775\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.27403#011validation-error:0.27428\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.26880#011validation-error:0.26840\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.26730#011validation-error:0.26716\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.26576#011validation-error:0.26572\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.26480#011validation-error:0.26480\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.26218#011validation-error:0.26207\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.26065#011validation-error:0.26067\u001b[0m\n",
      "\n",
      "2021-08-12 19:06:47 Uploading - Uploading generated training model\n",
      "2021-08-12 19:06:47 Completed - Training job completed\n",
      "Training seconds: 267\n",
      "Billable seconds: 267\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit({\"train\": train_input, \"validation\": validation_input}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug and Visualize Output\n",
    "Within this step, mostly for debugging and visualization purposes, we call to our output and confirm the files were created. Then we get the link to our html file which contains our confusion matrix and other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-12 19:07:36     617396 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/CreateXgboostReport/xgboost_report.html\n",
      "2021-08-12 19:07:36     418220 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/CreateXgboostReport/xgboost_report.ipynb\n",
      "2021-08-12 19:06:44     322348 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-report.html\n",
      "2021-08-12 19:06:44     168692 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-report.ipynb\n",
      "2021-08-12 19:06:41        191 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/BatchSize.json\n",
      "2021-08-12 19:06:41        199 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/CPUBottleneck.json\n",
      "2021-08-12 19:06:41        126 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/Dataloader.json\n",
      "2021-08-12 19:06:41        127 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/GPUMemoryIncrease.json\n",
      "2021-08-12 19:06:41        198 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/IOBottleneck.json\n",
      "2021-08-12 19:06:41        119 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/LoadBalancing.json\n",
      "2021-08-12 19:06:41        151 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/LowGPUUtilization.json\n",
      "2021-08-12 19:06:41        177 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/MaxInitializationTime.json\n",
      "2021-08-12 19:06:41        133 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n",
      "2021-08-12 19:06:41        464 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/OverallSystemUsage.json\n",
      "2021-08-12 19:06:41        156 output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/StepOutlier.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/CPUBottleneck.json to ProfilerReport-1628794782/profiler-output/profiler-reports/CPUBottleneck.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/Dataloader.json to ProfilerReport-1628794782/profiler-output/profiler-reports/Dataloader.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-report.ipynb to ProfilerReport-1628794782/profiler-output/profiler-report.ipynb\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/BatchSize.json to ProfilerReport-1628794782/profiler-output/profiler-reports/BatchSize.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/IOBottleneck.json to ProfilerReport-1628794782/profiler-output/profiler-reports/IOBottleneck.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-report.html to ProfilerReport-1628794782/profiler-output/profiler-report.html\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/GPUMemoryIncrease.json to ProfilerReport-1628794782/profiler-output/profiler-reports/GPUMemoryIncrease.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/LoadBalancing.json to ProfilerReport-1628794782/profiler-output/profiler-reports/LoadBalancing.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/CreateXgboostReport/xgboost_report.html to CreateXgboostReport/xgboost_report.html\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/LowGPUUtilization.json to ProfilerReport-1628794782/profiler-output/profiler-reports/LowGPUUtilization.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/CreateXgboostReport/xgboost_report.ipynb to CreateXgboostReport/xgboost_report.ipynb\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/MaxInitializationTime.json to ProfilerReport-1628794782/profiler-output/profiler-reports/MaxInitializationTime.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/StepOutlier.json to ProfilerReport-1628794782/profiler-output/profiler-reports/StepOutlier.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/OverallFrameworkMetrics.json to ProfilerReport-1628794782/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n",
      "download: s3://bucket-sagemaker-substance-abuse/output/models/xgboost_model/sagemaker-xgboost-2021-08-12-18-59-42-514/rule-output/ProfilerReport-1628794782/profiler-output/profiler-reports/OverallSystemUsage.json to ProfilerReport-1628794782/profiler-output/profiler-reports/OverallSystemUsage.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Click link below to view the XGBoost Training report'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='CreateXgboostReport/xgboost_report.html' target='_blank'>CreateXgboostReport/xgboost_report.html</a><br>"
      ],
      "text/plain": [
       "/root/predicting-the-future-TEDS/CreateXgboostReport/xgboost_report.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rule_output_path = xgb_model.output_path + \"/\" + xgb_model.latest_training_job.name + \"/rule-output\"\n",
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive\n",
    "from IPython.display import FileLink, FileLinks\n",
    "display(\"Click link below to view the XGBoost Training report\", FileLink(\"CreateXgboostReport/xgboost_report.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goals\n",
    "- [x] Get data into sagemaker\n",
    "- [x] Clean Data\n",
    "- [x] Transform Data\n",
    "- [x] Split Data\n",
    "- [x] Train Model\n",
    "- [ ] Modify Patterns and remove rows with now extraneous labels\n",
    "- [ ] Reach 80-85% Accuracy over the next 3 days\n",
    "- [ ] Deploy Model\n",
    "- [ ] Create Demo?"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Base Python)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/python-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
