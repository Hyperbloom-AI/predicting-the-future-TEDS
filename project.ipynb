{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substance Abuse Project\n",
    "\n",
    "To be able to predict the outcome of substance abuse treatment is a high value solution. A model being able to make the distinction between a passing and failing case is difficult and requires alot of information. For this iteration, we utlized the TEDs 2018 dataset provided by the **_Center for Behavioral Health Statistics and Quality_** and the **_Substance Abuse and Mental Health Services Administration_**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "If necessary please uncomment the following lines and install the dependencies with pip. Our notebook uses each one of these and it's important that all can be imported to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sagemaker\n",
    "# !pip install matplotlib\n",
    "# !pip install boto3\n",
    "# !pip install pandas\n",
    "# !pip install fsspec\n",
    "# !pip install s3fs\n",
    "# !pip install sklearn\n",
    "# !pip install seaborn\n",
    "# !pip install progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "For this section we work on removing uninmportant data, unlabelled rows, and modifying our dataset to be used with the XGBoost training algorithm.\n",
    "### Importing Dependencies and Setting Environment Variables\n",
    "In this next code block we import boto3 and pandas. Pandas will be important for our first step, importing the CSV file as a **dataframe** a sort of programatically accessible table that we can use in our python code. boto3 will become important in a later step when we need to export our split data back to s3. Though there are other popular dataframe modules such as dask, pandas remains the most open source with the best source documentation and clear instruction. It also features the fastest operation time. We also import math, json, numpy and matplotlib.pyplot for later use. Below that we set our environment variables which we use throughout the notebook.\n",
    "\n",
    "We also save our important filenames and the name of our bucket to the notebook for use later. Putting all this data together in one step means avoiding running extraneous code when rerunning code in later sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, pandas as pd, math, json, numpy as np, matplotlib.pyplot as plt, os\n",
    "\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "bucket = 'bucket-sagemaker-substance-abuse'\n",
    "raw_file_2017 = 'data/raw/tedsd_puf_2017.csv'\n",
    "raw_file_2018 = 'data/raw/tedsd_puf_2018.csv'\n",
    "train_subfolder = 'data/train/'\n",
    "eval_subfolder = 'data/evaluation/'\n",
    "label = \"REASON\" # Desired Label\n",
    "objective = 'binary:logistic' # binary:logistic or multi:softmax\n",
    "training_rounds = 100 # 1000-1500 for production, 10-100 for testing and development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our CSV as a pandas Dataframe Object\n",
    "Most of the lines in this block format a string for use with pandas, telling it where our file is located. I've opted for *join concatenation with a slash* instead of operator concatenation which uses more memory and is less efficient. The latter half of this block of code reads out the data from our source file to the pandas dataframe which we can mutate during our transformations. Any dataframes method with a possible `inplace` attribute should be set to true so that the dataframe is mutated and not returned except for in the case that you're saving it to a variable. \n",
    "\n",
    "This step may take a few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_location_2018 = 's3://{}/{}'.format(bucket, raw_file_2018)\n",
    "raw_location_2017 = 's3://{}/{}'.format(bucket, raw_file_2017)\n",
    "df_2018 = pd.read_csv(raw_location_2018)\n",
    "print(f\"2018 Dataframe Loaded: { len(df_2018) } rows x {len(df_2018.columns)} columns\")\n",
    "df_2017 = pd.read_csv(raw_location_2017)\n",
    "print(f\"2017 Dataframe Loaded: { len(df_2017) } rows x {len(df_2017.columns)} columns\")\n",
    "df = pd.concat([df_2018, df_2017])\n",
    "print(f\"Dataframe Loaded: { len(df) } rows x {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extraneous Columns\n",
    "In the next step we take the names of all the columns we want to remove and set them in an array. Originally we would use an array to loop through and drop them one by one however pandas `drop` function can take *list-like* parameters and so we can actually save both time and resources by doing it all at once. This will significantly increase development speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [\"CASEID\", \"EMPLOY_D\", \"DETNLF_D\", \"LIVARAG_D\", \"ARRESTS_D\", \"SERVICES_D\", \"SUB1_D\", \"FREQ1_D\", \"SUB2_D\", \"FREQ2_D\", \"SUB3_D\", \"FREQ3_D\", \"FREQ_ATND_SELF_HELP_D\", \"LOS\", \"DISYR\"]\n",
    "data_top = df.columns\n",
    "    \n",
    "# display \n",
    "print(data_top)\n",
    "index_names = df[ (df['REASON'] == 3) | (df['REASON'] == 5) | (df['REASON'] == 6) | (df['REASON'] == 7)].index # Collects Death, Incarceration, Termination, and Other labeled rows for removal\n",
    "df.drop(index_names, inplace = True) # Drops rows grabbed above\n",
    "df.drop(remove, inplace=True, axis=1) # Drops extraneous columns in 'remove' array above\n",
    "df.dropna(axis=0, subset=[label], inplace=True) # Drops rows missing desired label\n",
    "print(f\"Dataframe Transformed: { len(df) } rows x {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Categorical Data to Binary Columns\n",
    "<span style=\"color:red\">**BUG**</span> For this step we transform our categorical data into binary columns. To do this we call `get_dummies` on each column listed in the transform array as we loop through it. We then take that *dummy* dataframe and we concatenate it with the current dataframe along the vertical axis. After we loop through the entire array, we drop all the coulmns in the transform array as they're now redundant. Because there is a bug in this step, probably caused by a memory shortage or a CPU resource shortage, we should think about creating an empty dataframe in which we concatenate all dummies together and then concatenate that at the end instead of many times during the loop. In the last update I removed \"Reason\" from the transform list as we'll need to know what that becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = [\"AGE\", \"GENDER\", \"RACE\", \"ETHNIC\", \"MARSTAT\", \"EDUC\", \"EMPLOY\", \"DETNLF\", \"PREG\", \"VET\", \"LIVARAG\", \"PRIMINC\", \"ARRESTS\", \"SERVICES\", \"METHUSE\", \"DAYWAIT\", \"PSOURCE\", \"DETCRIM\", \"NOPRIOR\", \"SUB1\", \"ROUTE1\", \"FREQ1\", \"FRSTUSE1\", \"SUB2\", \"ROUTE2\", \"FREQ2\", \"FRSTUSE2\", \"SUB3\", \"ROUTE3\", \"FREQ3\", \"FRSTUSE3\", \"IDU\", \"DIVISION\", \"REGION\", \"STFIPS\"];\n",
    "df = pd.get_dummies(df, prefix=transform, columns=transform)\n",
    "print(f\"Dataframe Transformed: { len(df) } rows x {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Binary Classification and Modify Data for Algorithm\n",
    "This code moves our label to the front of the dataframe. It also decrements 1 from the Reason category so the the XDGBoost Algorithm can understand the classes. Our current categories are:\n",
    "- **0**: Positive Result\n",
    "- **1**: Non-positive Result\n",
    "\n",
    "For reasons having to do with confidence, we've phased out multi classification, though it can be reenabled by modifying the hyperparameters for the algorithm and modifying replace statements below. THe replace methods called may seem redundant but they are created in a way so that commenting out certain statements allows us to roll back to multi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label] = df[label].replace([3, 4, 2], [0, 0, 0]) # Converts to Binary\n",
    "column = df.pop(label)\n",
    "df.insert(0, label, column)\n",
    "class_pool = pd.get_dummies(df[label]) # For Debugging\n",
    "print(f\"Labels altered, class pool reduced to {len(class_pool.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "To do proper model training we'll use a holdout dataset for evaluation and so we'll need to split the set into 80% and 20%\n",
    "### Separating Training and Evaluation Sets and Uploading them to S3\n",
    "To complete our tranformation section, we grab a random sample of our data that makes up half of our data and we transport it to S3 utilizing a boto3 s3 client after converting the new dataframe to a CSV. If the command is run twice it will overwrite previous data. This section also creates an Evaluation Dataframe and ships it in a CSV to a seperate folder from the training data.\n",
    "\n",
    "This process will take up to 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.sample(frac=1)\n",
    "print(f\"Random Dataframe Created: { len(sdf) } rows x {len(sdf.columns)} columns\")\n",
    "training_end_index = math.floor(len(sdf)*0.8)\n",
    "eval_start_index = training_end_index + 1\n",
    "end = len(sdf) - 1\n",
    "tdf = sdf.iloc[0:training_end_index] # first five rows of dataframe\n",
    "print(f\"Training Dataframe Created: { len(tdf) } rows x {len(tdf.columns)} columns\")\n",
    "edf = sdf.iloc[training_end_index:end]\n",
    "print(f\"Evaluation Dataframe Created: { len(edf) } rows x {len(edf.columns)} columns\")\n",
    "\n",
    "t_filename = 'training_data.csv'\n",
    "e_filename = 'evaluation_data.csv'\n",
    "tdf.to_csv(t_filename, header=False, index=False)\n",
    "edf.to_csv(e_filename, header=False, index=False)\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file(t_filename, bucket, train_subfolder + t_filename)\n",
    "s3.meta.client.upload_file(e_filename, bucket, eval_subfolder + e_filename)\n",
    "print(f\"Operation Complete, Files Uploaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"deleting local files\")\n",
    "os.remove(t_filename)\n",
    "os.remove(e_filename)\n",
    "print(f\"local files deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (With the exception of block 1, the following can be independant)\n",
    "***\n",
    "## Testing The Model's Hyperparameters\n",
    "\n",
    "### Setting Sagemaker Environment\n",
    "Now we get to important stuff. Here we leave the transformation phase and move into the training phase. This block saves our region into the notebook making it usable in our model. The steps below do not need to be run in conjunction with those above, provided you've already completed the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Model Parameters and HyperParameters\n",
    "In this section we set up our XDGBoost training and pass it our basic values and hyperparameters. `num_round` determines how many rounds of training the model goes through. `num_class` alters how many classes it accepts. If the objective changes to binary this can be removed. Using A/B Testing I was able to discover there's not really a point to doing more than *1000* rounds of training. The training m-error may keep dropping but the evaluation m-error stays roughly stagnant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Region: us-east-2\n",
      "257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:1.2-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "model_bucket_prefix = 'output/models'\n",
    "\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, model_bucket_prefix, 'xgboost_model')\n",
    "\n",
    "container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "print(container)\n",
    "\n",
    "xgb_model=sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role = 'arn:aws:iam::620700586481:role/sagemaker-substance-abuse-project-role',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    volume_size=5,\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n",
    ")\n",
    "\n",
    "xgb_model.set_hyperparameters(\n",
    "    eval_metric=\"auc\",\n",
    "    max_depth = 6,\n",
    "    min_child_weight = 1,\n",
    "    rate_drop=0.3,\n",
    "    objective = objective\n",
    ")\n",
    "objective_metric_name = \"validation:auc\"\n",
    "\n",
    "prefix = \"data\"\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train/training_data.csv\"), content_type=\"csv\"\n",
    ")\n",
    "validation_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"evaluation/evaluation_data.csv\"), content_type=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hyperparameter Tuning\n",
    "For this part we'll begin hyperparameter tuning. For reference see [https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log.ipynb). We'll start with __Automatic Scaling__ and continue from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declaring Hyperparameter ranges\n",
      "Starting Automatic Tuning\n",
      ".........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "print(\"Declaring Hyperparameter ranges\")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"alpha\": ContinuousParameter(0.01, 10, scaling_type=\"Auto\"),\n",
    "    \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Auto\"),\n",
    "    \"min_child_weight\": ContinuousParameter(0.01, 120, scaling_type=\"Auto\"),\n",
    "    \"subsample\": ContinuousParameter(0.5, 1, scaling_type=\"Auto\"),\n",
    "    \"eta\": ContinuousParameter(0.1, 0.5, scaling_type=\"Auto\"),\n",
    "    \"num_round\": IntegerParameter(1, 4000, scaling_type=\"Auto\")\n",
    "}\n",
    "\n",
    "\n",
    "tuner_auto = HyperparameterTuner(\n",
    "    xgb_model,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=30,\n",
    "    max_parallel_jobs=3,\n",
    "    strategy=\"Random\"\n",
    ")\n",
    "\n",
    "print(\"Starting Automatic Tuning\")\n",
    "\n",
    "tuner_auto.fit(\n",
    "    {\"train\": train_input, \"validation\": validation_input}, \n",
    "    wait=True,\n",
    "    include_cls_metadata=False\n",
    ")\n",
    "\n",
    "boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner_auto.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]\n",
    "\n",
    "print(\"Automatic Tuning Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tuning Job Results and Training New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check jobs have finished\n",
    "auto_output = boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner_auto.latest_tuning_job.job_name\n",
    ")\n",
    "best_training_job = boto3.client(\"sagemaker\").describe_training_job(\n",
    "    TrainingJobName=auto_output[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    ")\n",
    "\n",
    "status_auto = auto_output[\"HyperParameterTuningJobStatus\"]\n",
    "\n",
    "assert status_auto == \"Completed\", \"First must be completed, was {}\".format(status_auto)\n",
    "\n",
    "df_auto = sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    tuner_auto.latest_tuning_job.job_name\n",
    ").dataframe()\n",
    "df_auto[\"scaling\"] = \"auto\"\n",
    "\n",
    "g = sns.FacetGrid(df_auto, col=\"scaling\", palette=\"viridis\")\n",
    "g = g.map(plt.scatter, \"alpha\", \"lambda\", alpha=0.6)\n",
    "\n",
    "# Set Best Hyperparameters\n",
    "\n",
    "best_hyperparams = best_training_job[\"HyperParameters\"]\n",
    "print(\"Best Accuracy: \" + str(best_training_job[\"FinalMetricDataList\"][0][\"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Production Level Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "predictor = tuner_auto.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge', \n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import sleep\n",
    "from progress.bar import Bar\n",
    "\n",
    "print(\"Importing NumPy\")\n",
    "def predict(data, rows=1000):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    with Bar('Loading', fill='@', suffix='%(percent).1f%% - %(eta)ds') as bar:\n",
    "        for array in split_array:\n",
    "            predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "            bar.next()\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "print(\"Created Prediction Definition\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Imported MatPlotLib.pyplot\")\n",
    "\n",
    "prefix = \"data\"\n",
    "\n",
    "raw_location = \"s3://{}/{}/{}\".format(bucket, prefix, \"evaluation/evaluation_data.csv\")\n",
    "test = pd.read_csv(raw_location)\n",
    "\n",
    "print(\"Created test dataframe\")\n",
    "\n",
    "print(\"Started Prediction\")\n",
    "predictions=predict(test.to_numpy()[:,1:])\n",
    "\n",
    "plt.hist(predictions)\n",
    "plt.show()\n",
    "\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "\n",
    "cutoff=0.5\n",
    "print(sklearn.metrics.confusion_matrix(test.iloc[:, 0], np.where(predictions > cutoff, 1, 0)))\n",
    "print(sklearn.metrics.classification_report(test.iloc[:, 0], np.where(predictions > cutoff, 1, 0)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "log_loss = []\n",
    "for c in cutoffs:\n",
    "    log_loss.append(\n",
    "        sklearn.metrics.log_loss(test.iloc[:, 0], np.where(predictions > c, 1, 0))\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(cutoffs, log_loss)\n",
    "plt.xlabel(\"Cutoff\")\n",
    "plt.ylabel(\"Log loss\")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    'Log loss is minimized at a cutoff of ', cutoffs[np.argmin(log_loss)], \n",
    "    ', and the log loss value at the minimum is ', np.min(log_loss)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dev Model For XGBoost Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "prefix = \"output/models\"\n",
    "\n",
    "s3_output_location='s3://{}/{}/{}'.format(bucket, prefix, 'xgboost_model')\n",
    "\n",
    "container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "print(container)\n",
    "\n",
    "xgb_model=sagemaker.estimator.Estimator(\n",
    "    image_uri=container,\n",
    "    role = 'arn:aws:iam::620700586481:role/sagemaker-substance-abuse-project-role',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    volume_size=5,\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n",
    ")\n",
    "\n",
    "rounds = best_hyperparams[\"num_round\"]\n",
    "\n",
    "print(f\"Objective will attempt to train with { rounds } training rounds expect a delay of {int((((27/100)*60) * int(rounds))/60)} minutes\")\n",
    "\n",
    "xgb_model.set_hyperparameters(\n",
    "    max_depth = best_hyperparams[\"max_depth\"],\n",
    "    eta = best_hyperparams[\"eta\"],\n",
    "    gamma = 4,\n",
    "    min_child_weight = best_hyperparams[\"min_child_weight\"],\n",
    "    subsample = best_hyperparams[\"subsample\"],\n",
    "    objective = best_hyperparams[\"objective\"],\n",
    "    num_round = best_hyperparams[\"num_round\"],\n",
    "    alpha = best_hyperparams[\"alpha\"],\n",
    "    reg_lambda = best_hyperparams[\"lambda\"],\n",
    "    rate_drop = best_hyperparams[\"rate_drop\"]\n",
    ")\n",
    "\n",
    "xgb_model.fit({\"train\": train_input, \"validation\": validation_input}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(xgb_model))\n",
    "\n",
    "rule_output_path = xgb_model.output_path + \"/\" + xgb_model.latest_training_job.name + \"/rule-output\"\n",
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive\n",
    "from IPython.display import FileLink, FileLinks\n",
    "display(\"Click link below to view the XGBoost Training report\", FileLink(\"CreateXgboostReport/xgboost_report.html\"))\n",
    "\n",
    "profiler_report_name = [rule[\"RuleConfigurationName\"] \n",
    "                        for rule in xgb_model.latest_training_job.rule_job_summary() \n",
    "                        if \"Profiler\" in rule[\"RuleConfigurationName\"]][0]\n",
    "profiler_report_name\n",
    "display(\"Click link below to view the profiler report\", FileLink(profiler_report_name+\"/profiler-output/profiler-report.html\"))\n",
    "\n",
    "xgb_model.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goals\n",
    "- [x] Get data into sagemaker\n",
    "- [x] Clean Data\n",
    "- [x] Transform Data\n",
    "- [x] Split Data\n",
    "- [x] Train Model\n",
    "- [x] Modify Patterns and remove rows with now extraneous labels\n",
    "- [x] Reach 80-85% Accuracy over the next 3 days\n",
    "- [x] Deploy Model\n",
    "- [ ] Create Demo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Base Python)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/python-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
